{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w2M2P-87hkur"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## import libraries and modules ##\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, TensorBoard\n",
        "\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWbgSDSMhkuu",
        "outputId": "fd774c06-69b5-400a-ada7-756b54722aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPU\n"
          ]
        }
      ],
      "source": [
        "## GPU use ##\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F0Y4dpVbhkuv"
      },
      "outputs": [],
      "source": [
        "## Tensorboard ##\n",
        "%load_ext tensorboard\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11fMIgo6jI8t",
        "outputId": "5c1281df-5812-46ff-9ebe-1bbf06e5c7ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "## Access Google Drive ##\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmn4BOsyhkuw",
        "outputId": "434557d5-a4f5-4fd4-e62a-258582edd77a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((3658, 120, 1), (915, 120, 1), (3658,), (915,))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Data preprocessing ##\n",
        "\n",
        "# Load & extract data\n",
        "data_rain = pd.read_csv('/content/drive/MyDrive/data_mrc/Rainfall_StungTreng.csv')\n",
        "data_flow = pd.read_csv('/content/drive/MyDrive/data_mrc/Discharge_StungTreng.csv')\n",
        "\n",
        "data_rain = data_rain.rename(columns={'Value': 'rainfall'})\n",
        "data_flow = data_flow.rename(columns={'Value': 'flowrate'})\n",
        "\n",
        "data_rain = data_rain[['Timestamp (UTC+07:00)', 'rainfall']]\n",
        "data_flow = data_flow[['Timestamp (UTC+07:00)', 'flowrate']]\n",
        "\n",
        "# Merge data\n",
        "data = pd.merge(data_rain, data_flow, on='Timestamp (UTC+07:00)', how='inner')\n",
        "\n",
        "# Normalize data\n",
        "scaler = MinMaxScaler()\n",
        "data[['rainfall', 'flowrate']] = scaler.fit_transform(data[['rainfall', 'flowrate']])\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(input_data, target_data, sequence_length):\n",
        "\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for i in range(len(input_data)-sequence_length):\n",
        "        xs.append(input_data.iloc[i:(i+sequence_length)].values)\n",
        "        ys.append(target_data.iloc[i+sequence_length])\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 120  # Day series length to consider for prediction\n",
        "X, y = create_sequences(data[['rainfall']], data['flowrate'], seq_length)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNA6fheXhkux"
      },
      "outputs": [],
      "source": [
        "## Models ##\n",
        "\n",
        "model = Sequential([\n",
        "    Input((seq_length, 1)),\n",
        "    LSTM(50, activation='tanh'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=[tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "model2 = Sequential([\n",
        "    Input((seq_length, 1)),\n",
        "    Bidirectional(LSTM(50, activation='tanh', return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50, activation='tanh'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model2.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=[tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "model3 = SARIMAX(data['flowrate'], order=(1, 1, 1), seasonal_order= (1, 1, 1, 12), exog= data['rainfall'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keF5lvewhkuy"
      },
      "outputs": [],
      "source": [
        "## Training and evaluation ##\n",
        "\n",
        "# Training\n",
        "history = model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnpvrHoyhkuy"
      },
      "outputs": [],
      "source": [
        "## Model 2 training with early stopping ##\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
        "\n",
        "# Learning rate decay function\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * np.exp(-0.1)\n",
        "\n",
        "#lr_scheduler = LearningRateScheduler(scheduler)\n",
        "history2 = model2.fit(X_train, y_train, epochs=500, batch_size=32, validation_split=0.1, callbacks=[early_stopping , tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi4-3PB7hkuz"
      },
      "outputs": [],
      "source": [
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model 1 Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cyT6tnuhkuz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history2.history['loss'], label='Training Loss')\n",
        "plt.plot(history2.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model 2 Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agjAhJb1hku0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history3.history['loss'], label='Training Loss')\n",
        "plt.plot(history3.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model 3 Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lnsVZUVhku0"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "loss2 = model2.evaluate(X_test, y_test)\n",
        "print(f'Test Loss 2: {loss2}')\n",
        "\n",
        "\n",
        "# Prediction\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred2 = model2.predict(X_test)\n",
        "\n",
        "# Calculate MSE and MAE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "mse2 = mean_squared_error(y_test, y_pred2)\n",
        "mae2 = mean_absolute_error(y_test, y_pred2)\n",
        "\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "print(f'Mean Squared Error 2: {mse2}')\n",
        "print(f'Mean Absolute Error 2: {mae2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm-xLNO4hku0"
      },
      "outputs": [],
      "source": [
        "## Plot ##\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test[550:600], label='True')\n",
        "plt.plot(y_pred[550:600], label='Predicted')\n",
        "#plt.plot(y_pred2[550:600], label='Predicted 2')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Flowrate')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "## Save model ##\n",
        "model.save('model.h5')\n",
        "model2.save('model2.h5')\n",
        "#model3.save('model_sarimax.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeZ3luY-vBvB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
